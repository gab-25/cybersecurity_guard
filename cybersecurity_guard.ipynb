{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T10:20:26.379598Z",
     "start_time": "2025-09-06T10:20:24.615364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Installazione libreria gym-idsgame\n",
    "!SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True pip install gym-idsgame"
   ],
   "id": "99362cd91372ff9b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym-idsgame in ./.venv/lib/python3.13/site-packages (1.0.12)\r\n",
      "Requirement already satisfied: gym in ./.venv/lib/python3.13/site-packages (from gym-idsgame) (0.26.2)\r\n",
      "Requirement already satisfied: pyglet in ./.venv/lib/python3.13/site-packages (from gym-idsgame) (2.1.8)\r\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (from gym-idsgame) (2.2.6)\r\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.13/site-packages (from gym-idsgame) (2.8.0)\r\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.13/site-packages (from gym-idsgame) (3.10.6)\r\n",
      "Requirement already satisfied: seaborn in ./.venv/lib/python3.13/site-packages (from gym-idsgame) (0.13.2)\r\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.13/site-packages (from gym-idsgame) (4.67.1)\r\n",
      "Requirement already satisfied: opencv-python in ./.venv/lib/python3.13/site-packages (from gym-idsgame) (4.12.0.88)\r\n",
      "Requirement already satisfied: imageio in ./.venv/lib/python3.13/site-packages (from gym-idsgame) (2.37.0)\r\n",
      "Requirement already satisfied: jsonpickle in ./.venv/lib/python3.13/site-packages (from gym-idsgame) (4.1.1)\r\n",
      "Requirement already satisfied: tensorboard in ./.venv/lib/python3.13/site-packages (from gym-idsgame) (2.20.0)\r\n",
      "Requirement already satisfied: sklearn in ./.venv/lib/python3.13/site-packages (from gym-idsgame) (0.0.post12)\r\n",
      "Requirement already satisfied: stable-baselines3 in ./.venv/lib/python3.13/site-packages (from gym-idsgame) (2.7.0)\r\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.13/site-packages (from gym-idsgame) (0.23.0)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./.venv/lib/python3.13/site-packages (from gym->gym-idsgame) (3.1.1)\r\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in ./.venv/lib/python3.13/site-packages (from gym->gym-idsgame) (0.1.0)\r\n",
      "Requirement already satisfied: pillow>=8.3.2 in ./.venv/lib/python3.13/site-packages (from imageio->gym-idsgame) (11.3.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.13/site-packages (from matplotlib->gym-idsgame) (1.3.3)\r\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.13/site-packages (from matplotlib->gym-idsgame) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.13/site-packages (from matplotlib->gym-idsgame) (4.59.2)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.13/site-packages (from matplotlib->gym-idsgame) (1.4.9)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from matplotlib->gym-idsgame) (25.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.13/site-packages (from matplotlib->gym-idsgame) (3.2.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.13/site-packages (from matplotlib->gym-idsgame) (2.9.0.post0)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib->gym-idsgame) (1.17.0)\r\n",
      "Requirement already satisfied: pandas>=1.2 in ./.venv/lib/python3.13/site-packages (from seaborn->gym-idsgame) (2.3.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas>=1.2->seaborn->gym-idsgame) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas>=1.2->seaborn->gym-idsgame) (2025.2)\r\n",
      "Requirement already satisfied: gymnasium<1.3.0,>=0.29.1 in ./.venv/lib/python3.13/site-packages (from stable-baselines3->gym-idsgame) (1.2.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./.venv/lib/python3.13/site-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3->gym-idsgame) (4.15.0)\r\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./.venv/lib/python3.13/site-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3->gym-idsgame) (0.0.4)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from torch->gym-idsgame) (3.19.1)\r\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch->gym-idsgame) (80.9.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.13/site-packages (from torch->gym-idsgame) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.13/site-packages (from torch->gym-idsgame) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch->gym-idsgame) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.13/site-packages (from torch->gym-idsgame) (2025.9.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.13/site-packages (from torch->gym-idsgame) (12.8.93)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.13/site-packages (from torch->gym-idsgame) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.13/site-packages (from torch->gym-idsgame) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.13/site-packages (from torch->gym-idsgame) (9.10.2.21)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.13/site-packages (from torch->gym-idsgame) (12.8.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.13/site-packages (from torch->gym-idsgame) (11.3.3.83)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.13/site-packages (from torch->gym-idsgame) (10.3.9.90)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.13/site-packages (from torch->gym-idsgame) (11.7.3.90)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.13/site-packages (from torch->gym-idsgame) (12.5.8.93)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.13/site-packages (from torch->gym-idsgame) (0.7.1)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./.venv/lib/python3.13/site-packages (from torch->gym-idsgame) (2.27.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.13/site-packages (from torch->gym-idsgame) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.13/site-packages (from torch->gym-idsgame) (12.8.93)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.13/site-packages (from torch->gym-idsgame) (1.13.1.3)\r\n",
      "Requirement already satisfied: triton==3.4.0 in ./.venv/lib/python3.13/site-packages (from torch->gym-idsgame) (3.4.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch->gym-idsgame) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch->gym-idsgame) (3.0.2)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in ./.venv/lib/python3.13/site-packages (from tensorboard->gym-idsgame) (2.3.1)\r\n",
      "Requirement already satisfied: grpcio>=1.48.2 in ./.venv/lib/python3.13/site-packages (from tensorboard->gym-idsgame) (1.74.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.venv/lib/python3.13/site-packages (from tensorboard->gym-idsgame) (3.9)\r\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in ./.venv/lib/python3.13/site-packages (from tensorboard->gym-idsgame) (6.32.0)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.venv/lib/python3.13/site-packages (from tensorboard->gym-idsgame) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.venv/lib/python3.13/site-packages (from tensorboard->gym-idsgame) (3.1.3)\r\n"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Algoritmo SARSA",
   "id": "58d2bd2b6493d65f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T07:47:21.859215Z",
     "start_time": "2025-09-09T07:47:10.969569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from gym_idsgame.agents.bot_agents.random_attack_bot_agent import RandomAttackBotAgent\n",
    "from gym_idsgame.envs import IdsGameRandomAttackV21Env, IdsGameMaximalAttackV21Env\n",
    "from gym_idsgame.envs.dao.game_config import GameConfig\n",
    "from gym_idsgame.envs.dao.idsgame_config import IdsGameConfig\n",
    "\n",
    "# Configura il gioco e l'ambiente per scenario Random Attack (noi controlliamo il difensore)\n",
    "# Nota: In IdsGameRandomAttackV21Env l'attaccante è parte dell'ambiente; noi forniamo eventualmente un bot\n",
    "# per l'attaccante, ma la nostra politica da apprendere è quella del difensore.\n",
    "\n",
    "# Creazione dell'ambiente con attaccante casuale\n",
    "base_game_config = GameConfig()\n",
    "attacker_agent = RandomAttackBotAgent(game_config=base_game_config, env=None)\n",
    "idsgame_config = IdsGameConfig(game_config=base_game_config, attacker_agent=attacker_agent)\n",
    "env = IdsGameRandomAttackV21Env(idsgame_config=idsgame_config)\n",
    "\n",
    "# Iperparametri SARSA\n",
    "alpha = 0.5      # tasso di apprendimento\n",
    "gamma = 0.95     # fattore di sconto\n",
    "epsilon = 0.1    # esplorazione\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.9995\n",
    "num_episodes = 2000  # per esecuzioni locali rapide; aumentare in Colab (es. 50k)\n",
    "\n",
    "# Osservazioni e spazi di azione (difensore)\n",
    "# L'osservazione è un array numpy; useremo una tabella Q basata su hashing dello stato per semplicità.\n",
    "num_actions = env.defender_action_space.n\n",
    "Q = {}  # dizionario: chiave=(state_tuple), valore=np.array di dimensione num_actions\n",
    "\n",
    "def state_key(obs: np.ndarray):\n",
    "    # Converti l'osservazione in una tupla hashable a bassa collisione\n",
    "    return tuple(np.asarray(obs, dtype=np.int16).ravel().tolist())\n",
    "\n",
    "\n",
    "def get_Q_row(skey):\n",
    "    if skey not in Q:\n",
    "        Q[skey] = np.zeros(num_actions, dtype=np.float32)\n",
    "    return Q[skey]\n",
    "\n",
    "\n",
    "def choose_action_from_Q(skey, eps):\n",
    "    if np.random.rand() < eps:\n",
    "        return np.random.randint(0, num_actions)\n",
    "    qrow = get_Q_row(skey)\n",
    "    return int(np.argmax(qrow))\n",
    "\n",
    "\n",
    "# Ciclo di addestramento SARSA (on-policy)\n",
    "print(\"Avvio dell'addestramento con SARSA (difensore vs random attacker)...\")\n",
    "for episode in range(num_episodes):\n",
    "    obs, reward, done, info = env.reset(), (0, 0), False, {}\n",
    "    skey = state_key(obs)\n",
    "    action = choose_action_from_Q(skey, epsilon)\n",
    "\n",
    "    episode_return_d = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    while not done:\n",
    "        # In DefenderEnv, step richiede una coppia (attacco, difesa); l'attacco è generato dall'ambiente,\n",
    "        # quindi si passa -1 per l'attacco e l'azione di difesa come intero.\n",
    "        next_obs, r, done, _ = env.step((-1, action))\n",
    "        # r è una tupla (reward_attacker, reward_defender)\n",
    "        r_d = r[1]\n",
    "        episode_return_d += r_d\n",
    "\n",
    "        next_skey = state_key(next_obs)\n",
    "        next_action = choose_action_from_Q(next_skey, epsilon)\n",
    "\n",
    "        # Aggiornamento SARSA: Q(s,a) <- Q(s,a) + alpha * [r + gamma*Q(s',a') - Q(s,a)]\n",
    "        qsa = get_Q_row(skey)\n",
    "        qsa_next = get_Q_row(next_skey)\n",
    "        td_target = r_d + (0 if done else gamma * qsa_next[next_action])\n",
    "        td_error = td_target - qsa[action]\n",
    "        qsa[action] += alpha * td_error\n",
    "\n",
    "        skey = next_skey\n",
    "        action = next_action\n",
    "        steps += 1\n",
    "\n",
    "    # Decadimento epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f\"Episodio {episode+1}: ritorno difensore = {episode_return_d:.1f}, epsilon = {epsilon:.3f}\")\n",
    "\n",
    "print(\"\\nAddestramento completato.\")\n",
    "\n",
    "# Valutazione della politica appresa (modalità greedy)\n",
    "print(\"\\nValutazione della politica appresa...\")\n",
    "num_test_episodes = 50\n",
    "total_test_rewards_d = 0.0\n",
    "for _ in range(num_test_episodes):\n",
    "    obs, done = env.reset(), False\n",
    "    while not done:\n",
    "        skey = state_key(obs)\n",
    "        qrow = get_Q_row(skey)\n",
    "        action = int(np.argmax(qrow))\n",
    "        obs, r, done, _ = env.step((-1, action))\n",
    "        total_test_rewards_d += r[1]\n",
    "\n",
    "print(f\"Ricompensa media (difensore) su {num_test_episodes} episodi di test: {total_test_rewards_d / num_test_episodes:.2f}\")"
   ],
   "id": "a3ff4acee3a0fbf4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avvio dell'addestramento con SARSA (difensore vs random attacker)...\n",
      "Episodio 100: ritorno difensore = -100.0, epsilon = 0.095\n",
      "Episodio 200: ritorno difensore = -100.0, epsilon = 0.090\n",
      "Episodio 300: ritorno difensore = -100.0, epsilon = 0.086\n",
      "Episodio 400: ritorno difensore = -100.0, epsilon = 0.082\n",
      "Episodio 500: ritorno difensore = -100.0, epsilon = 0.078\n",
      "Episodio 600: ritorno difensore = -100.0, epsilon = 0.074\n",
      "Episodio 700: ritorno difensore = -100.0, epsilon = 0.070\n",
      "Episodio 800: ritorno difensore = -100.0, epsilon = 0.067\n",
      "Episodio 900: ritorno difensore = -100.0, epsilon = 0.064\n",
      "Episodio 1000: ritorno difensore = -100.0, epsilon = 0.061\n",
      "Episodio 1100: ritorno difensore = -100.0, epsilon = 0.058\n",
      "Episodio 1200: ritorno difensore = -100.0, epsilon = 0.055\n",
      "Episodio 1300: ritorno difensore = -100.0, epsilon = 0.052\n",
      "Episodio 1400: ritorno difensore = -100.0, epsilon = 0.050\n",
      "Episodio 1500: ritorno difensore = -100.0, epsilon = 0.047\n",
      "Episodio 1600: ritorno difensore = -100.0, epsilon = 0.045\n",
      "Episodio 1700: ritorno difensore = -100.0, epsilon = 0.043\n",
      "Episodio 1800: ritorno difensore = -100.0, epsilon = 0.041\n",
      "Episodio 1900: ritorno difensore = -100.0, epsilon = 0.039\n",
      "Episodio 2000: ritorno difensore = -100.0, epsilon = 0.037\n",
      "\n",
      "Addestramento completato.\n",
      "\n",
      "Valutazione della politica appresa...\n",
      "Ricompensa media (difensore) su 50 episodi di test: -100.00\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Algoritmo DDQN",
   "id": "5f2ee9ecce2d7db7"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-09-09T07:47:33.183265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "\n",
    "# Funzioni di supporto per DDQN\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity:int):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "    def sample(self, batch_size:int):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        return Transition(*zip(*batch))\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim:int, output_dim:int):\n",
    "        super().__init__()\n",
    "        hidden = 256\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Utility per ottenere dimensione stato\n",
    "def flatten_obs(obs):\n",
    "    arr = np.asarray(obs, dtype=np.float32).ravel()\n",
    "    return arr\n",
    "\n",
    "# Costruzione env per DDQN (RandomAttack e MaximalAttack)\n",
    "base_game_config_ddqn = GameConfig()\n",
    "attacker_bot_ddqn = RandomAttackBotAgent(game_config=base_game_config_ddqn, env=None)\n",
    "idsgame_config_ddqn = IdsGameConfig(game_config=base_game_config_ddqn, attacker_agent=attacker_bot_ddqn)\n",
    "ra_env = IdsGameRandomAttackV21Env(idsgame_config=idsgame_config_ddqn)\n",
    "ma_env = IdsGameMaximalAttackV21Env(idsgame_config=idsgame_config_ddqn)\n",
    "\n",
    "# Parametri DDQN\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "ddqn_gamma = 0.99\n",
    "lr = 1e-3\n",
    "buffer_capacity = 100_000\n",
    "batch_size = 128\n",
    "target_update_freq = 1000  # passi di training\n",
    "start_learning_after = 1000  # passi di esperienza prima di iniziare update\n",
    "train_steps = 50_000  # per Colab aumentare (es. 500k)\n",
    "max_steps_per_episode = 500\n",
    "\n",
    "# Epsilon schedule\n",
    "eps_start = 1.0\n",
    "eps_end = 0.05\n",
    "eps_decay_steps = 30_000\n",
    "\n",
    "def epsilon_by_step(t):\n",
    "    if t >= eps_decay_steps:\n",
    "        return eps_end\n",
    "    frac = t / eps_decay_steps\n",
    "    return eps_start + (eps_end - eps_start) * frac\n",
    "\n",
    "# Setup rete per RandomAttack\n",
    "obs_dim = flatten_obs(ra_env.reset()).shape[0]\n",
    "act_dim = ra_env.defender_action_space.n\n",
    "online_q = QNetwork(obs_dim, act_dim).to(device)\n",
    "target_q = QNetwork(obs_dim, act_dim).to(device)\n",
    "target_q.load_state_dict(online_q.state_dict())\n",
    "optimizer = optim.Adam(online_q.parameters(), lr=lr)\n",
    "replay = ReplayBuffer(buffer_capacity)\n",
    "\n",
    "# Funzione per selezione azione epsilon-greedy\n",
    "def select_action(net, obs, eps):\n",
    "    if random.random() < eps:\n",
    "        return random.randrange(act_dim)\n",
    "    with torch.no_grad():\n",
    "        s = torch.from_numpy(flatten_obs(obs)).unsqueeze(0).to(device)\n",
    "        q = net(s)\n",
    "        return int(q.argmax(dim=1).item())\n",
    "\n",
    "# Loop di training DDQN su RandomAttack\n",
    "print(\"\\n[DDQN] Training su RandomAttack (difensore)...\")\n",
    "state = ra_env.reset()\n",
    "train_t = 0\n",
    "episode_return = 0.0\n",
    "for step in range(train_steps):\n",
    "    eps = epsilon_by_step(train_t)\n",
    "    action = select_action(online_q, state, eps)\n",
    "    next_state, r, done, _ = ra_env.step((-1, action))\n",
    "    r_d = float(r[1])\n",
    "    replay.push(flatten_obs(state), action, r_d, flatten_obs(next_state), float(done))\n",
    "    state = next_state\n",
    "    episode_return += r_d\n",
    "\n",
    "    # update\n",
    "    if len(replay) >= start_learning_after:\n",
    "        batch = replay.sample(batch_size)\n",
    "        s = torch.from_numpy(np.stack(batch.state)).float().to(device)\n",
    "        a = torch.tensor(batch.action, dtype=torch.int64).unsqueeze(1).to(device)\n",
    "        r_t = torch.tensor(batch.reward, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        ns = torch.from_numpy(np.stack(batch.next_state)).float().to(device)\n",
    "        d = torch.tensor(batch.done, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "        # Q(s,a)\n",
    "        q_sa = online_q(s).gather(1, a)\n",
    "        # DDQN target: a* = argmax_a Q_online(s', a), target = r + gamma*(1-d)*Q_target(s', a*)\n",
    "        with torch.no_grad():\n",
    "            next_actions = online_q(ns).argmax(dim=1, keepdim=True)\n",
    "            q_next = target_q(ns).gather(1, next_actions)\n",
    "            target = r_t + ddqn_gamma * (1.0 - d) * q_next\n",
    "        loss = nn.SmoothL1Loss()(q_sa, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(online_q.parameters(), 10.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Aggiornamento periodico target\n",
    "        if train_t % target_update_freq == 0:\n",
    "            target_q.load_state_dict(online_q.state_dict())\n",
    "\n",
    "    train_t += 1\n",
    "\n",
    "    if done or (step % max_steps_per_episode == 0 and step > 0):\n",
    "        # reset episodio\n",
    "        print(f\"Step {step}: return episodio (difensore) = {episode_return:.1f}, eps={eps:.2f}\")\n",
    "        state = ra_env.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "print(\"[DDQN] Training RandomAttack completato.\")\n",
    "\n",
    "# Valutazione DDQN su RandomAttack\n",
    "def evaluate(env_eval, net, episodes=20):\n",
    "    total = 0.0\n",
    "    for _ in range(episodes):\n",
    "        obs, done = env_eval.reset(), False\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                s = torch.from_numpy(flatten_obs(obs)).unsqueeze(0).to(device)\n",
    "                a = int(net(s).argmax(1).item())\n",
    "            obs, r, done, _ = env_eval.step((-1, a))\n",
    "            total += float(r[1])\n",
    "    return total / episodes\n",
    "\n",
    "ra_score = evaluate(ra_env, online_q, episodes=20)\n",
    "print(f\"[DDQN] RandomAttack - Ricompensa media (difensore) su 20 episodi: {ra_score:.2f}\")\n",
    "\n",
    "# Re-setup rete per MaximalAttack (riuso pesi iniziali o trasferimento opzionale)\n",
    "obs_dim_ma = flatten_obs(ma_env.reset()).shape[0]\n",
    "act_dim_ma = ma_env.defender_action_space.n\n",
    "\n",
    "online_q_ma = QNetwork(obs_dim_ma, act_dim_ma).to(device)\n",
    "target_q_ma = QNetwork(obs_dim_ma, act_dim_ma).to(device)\n",
    "target_q_ma.load_state_dict(online_q_ma.state_dict())\n",
    "optimizer_ma = optim.Adam(online_q_ma.parameters(), lr=lr)\n",
    "replay_ma = ReplayBuffer(buffer_capacity)\n",
    "\n",
    "print(\"\\n[DDQN] Training su MaximalAttack (difensore)...\")\n",
    "state = ma_env.reset()\n",
    "train_t = 0\n",
    "episode_return = 0.0\n",
    "for step in range(train_steps):\n",
    "    eps = epsilon_by_step(train_t)\n",
    "    action = select_action(online_q_ma, state, eps)\n",
    "    next_state, r, done, _ = ma_env.step((-1, action))\n",
    "    r_d = float(r[1])\n",
    "    replay_ma.push(flatten_obs(state), action, r_d, flatten_obs(next_state), float(done))\n",
    "    state = next_state\n",
    "    episode_return += r_d\n",
    "\n",
    "    if len(replay_ma) >= start_learning_after:\n",
    "        batch = replay_ma.sample(batch_size)\n",
    "        s = torch.from_numpy(np.stack(batch.state)).float().to(device)\n",
    "        a = torch.tensor(batch.action, dtype=torch.int64).unsqueeze(1).to(device)\n",
    "        r_t = torch.tensor(batch.reward, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        ns = torch.from_numpy(np.stack(batch.next_state)).float().to(device)\n",
    "        d = torch.tensor(batch.done, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "        q_sa = online_q_ma(s).gather(1, a)\n",
    "        with torch.no_grad():\n",
    "            next_actions = online_q_ma(ns).argmax(dim=1, keepdim=True)\n",
    "            q_next = target_q_ma(ns).gather(1, next_actions)\n",
    "            target = r_t + ddqn_gamma * (1.0 - d) * q_next\n",
    "        loss = nn.SmoothL1Loss()(q_sa, target)\n",
    "        optimizer_ma.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(online_q_ma.parameters(), 10.0)\n",
    "        optimizer_ma.step()\n",
    "\n",
    "        if train_t % target_update_freq == 0:\n",
    "            target_q_ma.load_state_dict(online_q_ma.state_dict())\n",
    "\n",
    "    train_t += 1\n",
    "\n",
    "    if done or (step % max_steps_per_episode == 0 and step > 0):\n",
    "        print(f\"Step {step}: return episodio (difensore) = {episode_return:.1f}, eps={eps:.2f}\")\n",
    "        state = ma_env.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "print(\"[DDQN] Training MaximalAttack completato.\")\n",
    "\n",
    "ma_score = evaluate(ma_env, online_q_ma, episodes=20)\n",
    "print(f\"[DDQN] MaximalAttack - Ricompensa media (difensore) su 20 episodi: {ma_score:.2f}\")\n"
   ],
   "id": "2b6e142f438b75c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DDQN] Training su RandomAttack (difensore)...\n",
      "Step 101: return episodio (difensore) = -100.0, eps=1.00\n",
      "Step 203: return episodio (difensore) = -100.0, eps=0.99\n",
      "Step 305: return episodio (difensore) = -100.0, eps=0.99\n",
      "Step 407: return episodio (difensore) = -100.0, eps=0.99\n",
      "Step 500: return episodio (difensore) = 0.0, eps=0.98\n",
      "Step 602: return episodio (difensore) = -100.0, eps=0.98\n",
      "Step 704: return episodio (difensore) = -100.0, eps=0.98\n",
      "Step 806: return episodio (difensore) = -100.0, eps=0.97\n",
      "Step 908: return episodio (difensore) = -100.0, eps=0.97\n",
      "Step 1000: return episodio (difensore) = 0.0, eps=0.97\n",
      "Step 1102: return episodio (difensore) = -100.0, eps=0.97\n",
      "Step 1204: return episodio (difensore) = -100.0, eps=0.96\n",
      "Step 1306: return episodio (difensore) = -100.0, eps=0.96\n",
      "Step 1408: return episodio (difensore) = -100.0, eps=0.96\n",
      "Step 1500: return episodio (difensore) = 0.0, eps=0.95\n",
      "Step 1602: return episodio (difensore) = -100.0, eps=0.95\n",
      "Step 1704: return episodio (difensore) = -100.0, eps=0.95\n",
      "Step 1806: return episodio (difensore) = -100.0, eps=0.94\n",
      "Step 1908: return episodio (difensore) = -100.0, eps=0.94\n",
      "Step 2000: return episodio (difensore) = 0.0, eps=0.94\n",
      "Step 2102: return episodio (difensore) = -100.0, eps=0.93\n",
      "Step 2204: return episodio (difensore) = -100.0, eps=0.93\n",
      "Step 2306: return episodio (difensore) = -100.0, eps=0.93\n",
      "Step 2408: return episodio (difensore) = -100.0, eps=0.92\n",
      "Step 2500: return episodio (difensore) = 0.0, eps=0.92\n",
      "Step 2602: return episodio (difensore) = -100.0, eps=0.92\n",
      "Step 2704: return episodio (difensore) = -100.0, eps=0.91\n",
      "Step 2806: return episodio (difensore) = -100.0, eps=0.91\n",
      "Step 2908: return episodio (difensore) = -100.0, eps=0.91\n",
      "Step 3000: return episodio (difensore) = 0.0, eps=0.91\n",
      "Step 3102: return episodio (difensore) = -100.0, eps=0.90\n",
      "Step 3204: return episodio (difensore) = -100.0, eps=0.90\n",
      "Step 3306: return episodio (difensore) = -100.0, eps=0.90\n",
      "Step 3408: return episodio (difensore) = -100.0, eps=0.89\n",
      "Step 3500: return episodio (difensore) = 0.0, eps=0.89\n",
      "Step 3602: return episodio (difensore) = -100.0, eps=0.89\n",
      "Step 3704: return episodio (difensore) = -100.0, eps=0.88\n",
      "Step 3806: return episodio (difensore) = -100.0, eps=0.88\n",
      "Step 3908: return episodio (difensore) = -100.0, eps=0.88\n",
      "Step 4000: return episodio (difensore) = 0.0, eps=0.87\n",
      "Step 4102: return episodio (difensore) = -100.0, eps=0.87\n",
      "Step 4204: return episodio (difensore) = -100.0, eps=0.87\n",
      "Step 4306: return episodio (difensore) = -100.0, eps=0.86\n",
      "Step 4408: return episodio (difensore) = -100.0, eps=0.86\n",
      "Step 4500: return episodio (difensore) = 0.0, eps=0.86\n",
      "Step 4602: return episodio (difensore) = -100.0, eps=0.85\n",
      "Step 4704: return episodio (difensore) = -100.0, eps=0.85\n",
      "Step 4806: return episodio (difensore) = -100.0, eps=0.85\n",
      "Step 4908: return episodio (difensore) = -100.0, eps=0.84\n",
      "Step 5000: return episodio (difensore) = 0.0, eps=0.84\n",
      "Step 5102: return episodio (difensore) = -100.0, eps=0.84\n",
      "Step 5204: return episodio (difensore) = -100.0, eps=0.84\n",
      "Step 5306: return episodio (difensore) = -100.0, eps=0.83\n",
      "Step 5408: return episodio (difensore) = -100.0, eps=0.83\n",
      "Step 5500: return episodio (difensore) = 0.0, eps=0.83\n",
      "Step 5602: return episodio (difensore) = -100.0, eps=0.82\n",
      "Step 5704: return episodio (difensore) = -100.0, eps=0.82\n",
      "Step 5806: return episodio (difensore) = -100.0, eps=0.82\n",
      "Step 5908: return episodio (difensore) = -100.0, eps=0.81\n",
      "Step 6000: return episodio (difensore) = 0.0, eps=0.81\n",
      "Step 6102: return episodio (difensore) = -100.0, eps=0.81\n",
      "Step 6204: return episodio (difensore) = -100.0, eps=0.80\n",
      "Step 6306: return episodio (difensore) = -100.0, eps=0.80\n",
      "Step 6408: return episodio (difensore) = -100.0, eps=0.80\n",
      "Step 6500: return episodio (difensore) = 0.0, eps=0.79\n",
      "Step 6602: return episodio (difensore) = -100.0, eps=0.79\n",
      "Step 6704: return episodio (difensore) = -100.0, eps=0.79\n",
      "Step 6806: return episodio (difensore) = -100.0, eps=0.78\n",
      "Step 6908: return episodio (difensore) = -100.0, eps=0.78\n",
      "Step 7000: return episodio (difensore) = 0.0, eps=0.78\n",
      "Step 7102: return episodio (difensore) = -100.0, eps=0.78\n",
      "Step 7204: return episodio (difensore) = -100.0, eps=0.77\n",
      "Step 7306: return episodio (difensore) = -100.0, eps=0.77\n",
      "Step 7408: return episodio (difensore) = -100.0, eps=0.77\n",
      "Step 7500: return episodio (difensore) = 0.0, eps=0.76\n",
      "Step 7602: return episodio (difensore) = -100.0, eps=0.76\n",
      "Step 7704: return episodio (difensore) = -100.0, eps=0.76\n",
      "Step 7806: return episodio (difensore) = -100.0, eps=0.75\n",
      "Step 7908: return episodio (difensore) = -100.0, eps=0.75\n",
      "Step 8000: return episodio (difensore) = 0.0, eps=0.75\n",
      "Step 8102: return episodio (difensore) = -100.0, eps=0.74\n",
      "Step 8204: return episodio (difensore) = -100.0, eps=0.74\n",
      "Step 8306: return episodio (difensore) = -100.0, eps=0.74\n",
      "Step 8408: return episodio (difensore) = -100.0, eps=0.73\n",
      "Step 8500: return episodio (difensore) = 0.0, eps=0.73\n",
      "Step 8602: return episodio (difensore) = -100.0, eps=0.73\n",
      "Step 8704: return episodio (difensore) = -100.0, eps=0.72\n",
      "Step 8806: return episodio (difensore) = -100.0, eps=0.72\n",
      "Step 8908: return episodio (difensore) = -100.0, eps=0.72\n",
      "Step 9000: return episodio (difensore) = 0.0, eps=0.72\n",
      "Step 9102: return episodio (difensore) = -100.0, eps=0.71\n",
      "Step 9204: return episodio (difensore) = -100.0, eps=0.71\n",
      "Step 9306: return episodio (difensore) = -100.0, eps=0.71\n",
      "Step 9408: return episodio (difensore) = -100.0, eps=0.70\n",
      "Step 9500: return episodio (difensore) = 0.0, eps=0.70\n",
      "Step 9602: return episodio (difensore) = -100.0, eps=0.70\n",
      "Step 9704: return episodio (difensore) = -100.0, eps=0.69\n",
      "Step 9806: return episodio (difensore) = -100.0, eps=0.69\n",
      "Step 9908: return episodio (difensore) = -100.0, eps=0.69\n",
      "Step 10000: return episodio (difensore) = 0.0, eps=0.68\n",
      "Step 10102: return episodio (difensore) = -100.0, eps=0.68\n",
      "Step 10204: return episodio (difensore) = -100.0, eps=0.68\n",
      "Step 10306: return episodio (difensore) = -100.0, eps=0.67\n",
      "Step 10408: return episodio (difensore) = -100.0, eps=0.67\n",
      "Step 10500: return episodio (difensore) = 0.0, eps=0.67\n",
      "Step 10602: return episodio (difensore) = -100.0, eps=0.66\n",
      "Step 10704: return episodio (difensore) = -100.0, eps=0.66\n",
      "Step 10806: return episodio (difensore) = -100.0, eps=0.66\n",
      "Step 10908: return episodio (difensore) = -100.0, eps=0.65\n",
      "Step 11000: return episodio (difensore) = 0.0, eps=0.65\n",
      "Step 11102: return episodio (difensore) = -100.0, eps=0.65\n",
      "Step 11204: return episodio (difensore) = -100.0, eps=0.65\n",
      "Step 11306: return episodio (difensore) = -100.0, eps=0.64\n",
      "Step 11408: return episodio (difensore) = -100.0, eps=0.64\n",
      "Step 11500: return episodio (difensore) = 0.0, eps=0.64\n",
      "Step 11602: return episodio (difensore) = -100.0, eps=0.63\n",
      "Step 11704: return episodio (difensore) = -100.0, eps=0.63\n",
      "Step 11806: return episodio (difensore) = -100.0, eps=0.63\n",
      "Step 11908: return episodio (difensore) = -100.0, eps=0.62\n",
      "Step 12000: return episodio (difensore) = 0.0, eps=0.62\n",
      "Step 12102: return episodio (difensore) = -100.0, eps=0.62\n",
      "Step 12204: return episodio (difensore) = -100.0, eps=0.61\n",
      "Step 12306: return episodio (difensore) = -100.0, eps=0.61\n",
      "Step 12408: return episodio (difensore) = -100.0, eps=0.61\n",
      "Step 12500: return episodio (difensore) = 0.0, eps=0.60\n",
      "Step 12602: return episodio (difensore) = -100.0, eps=0.60\n",
      "Step 12704: return episodio (difensore) = -100.0, eps=0.60\n",
      "Step 12806: return episodio (difensore) = -100.0, eps=0.59\n",
      "Step 12908: return episodio (difensore) = -100.0, eps=0.59\n",
      "Step 13000: return episodio (difensore) = 0.0, eps=0.59\n",
      "Step 13102: return episodio (difensore) = -100.0, eps=0.59\n",
      "Step 13204: return episodio (difensore) = -100.0, eps=0.58\n",
      "Step 13306: return episodio (difensore) = -100.0, eps=0.58\n",
      "Step 13408: return episodio (difensore) = -100.0, eps=0.58\n",
      "Step 13500: return episodio (difensore) = 0.0, eps=0.57\n",
      "Step 13602: return episodio (difensore) = -100.0, eps=0.57\n",
      "Step 13704: return episodio (difensore) = -100.0, eps=0.57\n",
      "Step 13806: return episodio (difensore) = -100.0, eps=0.56\n",
      "Step 13908: return episodio (difensore) = -100.0, eps=0.56\n",
      "Step 14000: return episodio (difensore) = 0.0, eps=0.56\n",
      "Step 14102: return episodio (difensore) = -100.0, eps=0.55\n",
      "Step 14204: return episodio (difensore) = -100.0, eps=0.55\n",
      "Step 14306: return episodio (difensore) = -100.0, eps=0.55\n",
      "Step 14408: return episodio (difensore) = -100.0, eps=0.54\n",
      "Step 14500: return episodio (difensore) = 0.0, eps=0.54\n",
      "Step 14602: return episodio (difensore) = -100.0, eps=0.54\n",
      "Step 14704: return episodio (difensore) = -100.0, eps=0.53\n",
      "Step 14806: return episodio (difensore) = -100.0, eps=0.53\n",
      "Step 14908: return episodio (difensore) = -100.0, eps=0.53\n",
      "Step 15000: return episodio (difensore) = 0.0, eps=0.53\n",
      "Step 15102: return episodio (difensore) = -100.0, eps=0.52\n",
      "Step 15204: return episodio (difensore) = -100.0, eps=0.52\n",
      "Step 15306: return episodio (difensore) = -100.0, eps=0.52\n",
      "Step 15408: return episodio (difensore) = -100.0, eps=0.51\n",
      "Step 15500: return episodio (difensore) = 0.0, eps=0.51\n",
      "Step 15602: return episodio (difensore) = -100.0, eps=0.51\n",
      "Step 15704: return episodio (difensore) = -100.0, eps=0.50\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
